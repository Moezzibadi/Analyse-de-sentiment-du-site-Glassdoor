---
title: "| <font size='15'>  Analyse de sentiments : \n| Temoignages des employés sur
  le site de *Glassdoor*</font> \n"
author: 'Présenté par : Mohammad MOEZZIBADI'
date: "\n \n \n \n Novembre 2021"
output:
  slidy_presentation: default
  ioslides_presentation: default
  beamer_presentation:
    latex_engine: xelatex
    includes:
      in_header: preamble.tex
  pdf_document: default
institute: Programmation avancées
bibliography: references.bib
theme: Madrid
classoption: aspectratio=169
header-includes:
- \usepackage{bm}
- \widowpenalties 1 150
- \usepackage{amsmath}
- \renewcommand{\bibname}{References}
- \usepackage{booktabs}
- \usepackage{tikz}
- \usepackage{verbatim}
- \usepackage{chronosys}
- \usepackage{stackengine}
- \usepackage[table, svgnames, dvipsnames]{xcolor}
- \usetikzlibrary{arrows.meta}
- \usetikzlibrary{arrows,decorations.markings}
- \usetikzlibrary{intersections}
- \usepackage[font=small]{caption}
- \usepackage{adjustbox}
- \usepackage{tabularx}
- \usepackage{pgfplots}
- \renewcommand{\bibname}{References}
- \usepackage{subcaption}
- \usepackage{wrapfig}
- \usepackage[most]{tcolorbox}
- \definecolor{light-yellow}{rgb}{1, 0.95, 0.7}
- \newtcolorbox{myquote}{colback=light-yellow,grow to right by=-10mm,grow to left by=-10mm,boxrule=0pt,boxsep=0pt,breakable}
- \newcommand{\todo}[1]{\begin{myquote} \textbf{Note :} \emph{#1} \end{myquote}
---


```{r setup, include=FALSE}
library(kableExtra)
library(magrittr) # For the %>% pipe operator
library(reticulate) # For Python
use_python("/Users/moezzibadi/Library/r-miniconda/envs/r-reticulate/bin/python", required=T)
py_install("bs4")
py_install("selenium")
knitr::opts_chunk$set(eval = TRUE)
python.reticulate = FALSE
def.chunk.hook  <- knitr::knit_hooks$get("chunk")
knitr::knit_hooks$set(chunk = function(x, options) {
  x <- def.chunk.hook(x, options)
  ifelse(options$size != "normalsize", paste0("\n \\", options$size,"\n\n", x, "\n\n \\normalsize"), x)
}, attr.output='style="max-height: 300px;"')
```
# Introduction

<center> 
![](Glassdoor.png){width=50%}
</center> 
<font size='6'>

- Une source d'informations très variées

- Les effets de l'évolution des salaires sur le chiffre d'affaires des entreprises

- Glassdoor est une source d'informations plus fiable que les documents officiels des entreprises (Tom Lakin-2015) 


<font size='6'>

# Problématique 

- **Avis ironiques** : 

<center> 
![Exemple d'avis](Review1.png){width=66%}
</center>

- **Un avis mélangé **: *Le poste [à l'entreprise] était horrible et la direction n'est pas capable de reconnaître les réalisations, mais une bonne opportunité de travailler avec mes collègues m'a incité à y aller tous les jours.*

# Plan général du code

<center>
![](Schema.png){width=60%}
</center>

# Plan général de scraping

- 16768 avis sur les entreprises situées en France qui recrutent des *data scientists*  

<center>
```{r, echo=FALSE,out.width="35%", out.height="12%",fig.cap=" Scraping ciblé des pages : I) configuration de la recherche II) changement des pages avec *selenium* III) un exemple de témoignages d'un employé",fig.show='hold',fig.align='center'}
knitr::include_graphics(c("Search.png","Review.png"))
``` 
</center>

# Construction de base de donées
- Construire un *dataframe* a partir du fichier *json* de résultats scrapés 

<center> 
![Automatisation dans le menu bar](CVC.png){width=70%}
</center>

```{python, size="tiny", eval=FALSE}
df = df.drop(['level_0', 'level_1'], axis=1)
df = df[0].apply(pd.Series)
...
df['Date'] = [','.join(map(str, l)) for l in df[2]]
df['Date'] = df['Date'].str.split('-', n = 1, expand = True)[0
to_replace = [' ','sept.','août', 'juil.', 'mai', "avr.", ..."]
replace_with = ['','sep','aug','jul', 'may' ,"apr", ...]
df['Date'] = df['Date'].replace(to_replace, replace_with, regex=True)
df = df.dropna(axis=0, how="any", thresh=None, subset=None, inplace=False)
df['Date'] = pd.to_datetime(df['Date'],format='%d%b%Y')
```

# Nuages de mots


```{r, echo=FALSE,out.width="40%", out.height="12%",fig.cap="Nuage des mots des avis **avant 2016** : positives (vert) et négatives (rouge)",fig.show='hold',fig.align='center'}
knitr::include_graphics(c("wordcloud.png","wordcloud1.png"))
``` 

```{r, echo=FALSE,out.width="40%", out.height="12%",fig.cap="Nuage des mots des avis **après 2016** : positives (vert) et négatives (rouge)",fig.show='hold',fig.align='center'}
knitr::include_graphics(c("wordcloud2.png","wordcloud3.png"))
``` 


# Classification de Naive Bayes 

<center>
<font size="10">$\color{red}{\displaystyle p(D\mid C)=\prod _{i}p(w_{i}\mid C)\,} \color{red}{=>} \color{red}{\displaystyle p(C\mid D)\,} \color{red}= \color{red}?$</font size>
</center>

<center>
$\begin{cases} {\displaystyle p(D\mid P)=\prod _{i}p(w_{i}\mid P)\,} \\ {\displaystyle p(D\mid \neg P)=\prod _{i}p(w_{i}\mid \neg P)\,}\end{cases}$ => Bayes' theorem : ${\displaystyle p(C\mid D)={\frac {p(C)\,p(D\mid C)}{p(D)}}}$
</center>

$$
\begin{cases} {\displaystyle p(P\mid D)={p(P) \over p(D)}\,\prod _{i}p(w_{i}\mid P)} \\ {\displaystyle p(\neg P\mid D)={p(\neg P) \over p(D)}\,\prod _{i}p(w_{i}\mid \neg P)} \end{cases} => {\displaystyle {p(P\mid D) \over p(\neg P\mid D)}={p(P) \over p(\neg P)}\,\prod _{i}{p(w_{i}\mid P) \over p(w_{i}\mid \neg P)}}
$$
<center>

${\displaystyle \ln {p(P\mid D) \over p(\neg P\mid D)}=\ln {p(P) \over p(\neg P)}+\sum _{i}\ln {p(w_{i}\mid P) \over p(w_{i}\mid \neg P)}}$

${\displaystyle p(P\mid D)>p(\neg P\mid D)}$ => ${\displaystyle \ln {p(P\mid D) \over p(\neg P\mid D)}>0}$
</center>

# Classification de Naive Bayes 

<center>
${\displaystyle p(D\mid C)=\prod _{i}p(w_{i}\mid C)\,}$ => ${\displaystyle p(C\mid D)\,} = ?$
</center>

<center>
<font size="6.5">$\color{red}{\begin{cases} \color{red}{\displaystyle p(D\mid P)=\prod _{i}p(w_{i}\mid P)\,} \\ \color{red}{\displaystyle p(D\mid \neg P)=\prod _{i}p(w_{i}\mid \neg P)\,}\end{cases}}$ $\color{red}{=> Bayes' \ theorem :}$ $\color{red}{\displaystyle p(C\mid D)={\frac {p(C)\,p(D\mid C)}{p(D)}}}$</font size>
</center>


$$
\begin{cases} {\displaystyle p(P\mid D)={p(P) \over p(D)}\,\prod _{i}p(w_{i}\mid P)} \\ {\displaystyle p(\neg P\mid D)={p(\neg P) \over p(D)}\,\prod _{i}p(w_{i}\mid \neg P)} \end{cases} => {\displaystyle {p(P\mid D) \over p(\neg P\mid D)}={p(P) \over p(\neg P)}\,\prod _{i}{p(w_{i}\mid P) \over p(w_{i}\mid \neg P)}}
$$
<center>

${\displaystyle \ln {p(P\mid D) \over p(\neg P\mid D)}=\ln {p(P) \over p(\neg P)}+\sum _{i}\ln {p(w_{i}\mid P) \over p(w_{i}\mid \neg P)}}$

${\displaystyle p(P\mid D)>p(\neg P\mid D)}$ => ${\displaystyle \ln {p(P\mid D) \over p(\neg P\mid D)}>0}$
</center>

# Classification de Naive Bayes 

<center>
${\displaystyle p(D\mid C)=\prod _{i}p(w_{i}\mid C)\,}$ => ${\displaystyle p(C\mid D)\,} = ?$
</center>

<center>
$\begin{cases} {\displaystyle p(D\mid P)=\prod _{i}p(w_{i}\mid P)\,} \\ {\displaystyle p(D\mid \neg P)=\prod _{i}p(w_{i}\mid \neg P)\,}\end{cases}$ => Bayes' theorem : ${\displaystyle p(C\mid D)={\frac {p(C)\,p(D\mid C)}{p(D)}}}$
</center>

<font size="6.5">$$
\color{red}{\begin{cases} {\displaystyle p(P\mid D)={p(P) \over p(D)}\,\prod _{i}p(w_{i}\mid P)} \\ {\displaystyle p(\neg P\mid D)={p(\neg P) \over p(D)}\,\prod _{i}p(w_{i}\mid \neg P)} \end{cases}} \color{red}{=>} \color{red}{\displaystyle {p(P\mid D) \over p(\neg P\mid D)}={p(P) \over p(\neg P)}\,\prod _{i}{p(w_{i}\mid P) \over p(w_{i}\mid \neg P)}}
$$</font size>
<center>

${\displaystyle \ln {p(P\mid D) \over p(\neg P\mid D)}=\ln {p(P) \over p(\neg P)}+\sum _{i}\ln {p(w_{i}\mid P) \over p(w_{i}\mid \neg P)}}$

${\displaystyle p(P\mid D)>p(\neg P\mid D)}$ => ${\displaystyle \ln {p(P\mid D) \over p(\neg P\mid D)}>0}$
</center>

# Classification de Naive Bayes 

<center>
${\displaystyle p(D\mid C)=\prod _{i}p(w_{i}\mid C)\,}$ => ${\displaystyle p(C\mid D)\,} = ?$
</center>

<center>
$\begin{cases} {\displaystyle p(D\mid P)=\prod _{i}p(w_{i}\mid P)\,} \\ {\displaystyle p(D\mid \neg P)=\prod _{i}p(w_{i}\mid \neg P)\,}\end{cases}$ => Bayes' theorem : ${\displaystyle p(C\mid D)={\frac {p(C)\,p(D\mid C)}{p(D)}}}$
</center>

$$
\begin{cases} {\displaystyle p(P\mid D)={p(P) \over p(D)}\,\prod _{i}p(w_{i}\mid P)} \\ {\displaystyle p(\neg P\mid D)={p(\neg P) \over p(D)}\,\prod _{i}p(w_{i}\mid \neg P)} \end{cases} => {\displaystyle {p(P\mid D) \over p(\neg P\mid D)}={p(P) \over p(\neg P)}\,\prod _{i}{p(w_{i}\mid P) \over p(w_{i}\mid \neg P)}}
$$
<center>

<font size="10">$\color{red}{\displaystyle \ln {p(P\mid D) \over p(\neg P\mid D)}=\ln {p(P) \over p(\neg P)}+\sum _{i}\ln {p(w_{i}\mid P) \over p(w_{i}\mid \neg P)}}$</font size>

${\displaystyle p(P\mid D)>p(\neg P\mid D)}$ => ${\displaystyle \ln {p(P\mid D) \over p(\neg P\mid D)}>0}$
</center>

# Classification de Naive Bayes 

<center>
${\displaystyle p(D\mid C)=\prod _{i}p(w_{i}\mid C)\,}$ => ${\displaystyle p(C\mid D)\,} = ?$
</center>

<center>
$\begin{cases} {\displaystyle p(D\mid P)=\prod _{i}p(w_{i}\mid P)\,} \\ {\displaystyle p(D\mid \neg P)=\prod _{i}p(w_{i}\mid \neg P)\,}\end{cases}$ => Bayes' theorem : ${\displaystyle p(C\mid D)={\frac {p(C)\,p(D\mid C)}{p(D)}}}$
</center>

$$
\begin{cases} {\displaystyle p(P\mid D)={p(P) \over p(D)}\,\prod _{i}p(w_{i}\mid P)} \\ {\displaystyle p(\neg P\mid D)={p(\neg P) \over p(D)}\,\prod _{i}p(w_{i}\mid \neg P)} \end{cases} => {\displaystyle {p(P\mid D) \over p(\neg P\mid D)}={p(P) \over p(\neg P)}\,\prod _{i}{p(w_{i}\mid P) \over p(w_{i}\mid \neg P)}}
$$
<center>

${\displaystyle \ln {p(P\mid D) \over p(\neg P\mid D)}=\ln {p(P) \over p(\neg P)}+\sum _{i}\ln {p(w_{i}\mid P) \over p(w_{i}\mid \neg P)}}$

<font size="10">$\color{red}{\displaystyle p(P\mid D)>p(\neg P\mid D)}$ $\color{red}{=>}$ $\color{red}{\displaystyle \ln {p(P\mid D) \over p(\neg P\mid D)}>0}$</font size>
</center>


# Classification de Naive Bayes 


**Module** :
 <font size='12'> 
```{python, eval=FALSE, size="large"}
from sklearn.naive_bayes import MultinomialNB
```
</font size> 
<center> 
![Matrice de confusion](CM.png){width=50%}
</center>

# Distribution de Dirichlet 
1) Non-Negative Matrix Factorization (NMF)
2) Latent Semantic Analysis or Latent Semantic Indexing (LSA or LSI) 
3) Latent Dirichlet Allocation

## *Latent Dirichlet Allocation* :

<font size='7'> $$ P(W,Z,\theta ,\varphi; \color{red}\alpha,\color{red}\beta) = \ \ \ \ \ \ \ \ \ \  \ \ \ \ \  \ \ \ \ \  \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \  \ \ \ \ \  \ \ \ \ \  \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \  \\ \Pi_{j=1}^{M}P(\theta_j;\color{red}\alpha)\ \Pi_{j=1}^{K}P(\varphi_i;\color{red}{\beta}) \ \Pi_{t=1}^{N}P(Z_{j,t}|\theta_j)P(W_{j,t}|\varphi_{Z_{j,t}})$$ </font>


- $\alpha$ \& $\beta$ — la distribution Dirichlet
- $\theta$ \& $\varphi$ —  la distribution multinomiale
- $Z$ — un ensemble de sujets
- $W$ — un ensemble de mots

# Distribution de Dirichlet 

<div class='left' style='float:left;width:40%'>
- $0< \alpha << 1$: 

    $(\color{red}{1.0} , 0.0, 0.0)$ ou 
    
    $(0.0, \color{red}{1.0}, 0.0)$ ou 
    
    $(0.0, 0.0, \color{red}{1.0})$

- $\alpha ≈ 1$:
    
    Distribution uniforme 
    
- $\alpha > 1$ :

    Une mixité égale des sujets:
    
    $(0.4, 0.3, 0.3)$

</div>
<div class='right' style='float:right;width:60%'>
![](https://miro.medium.com/max/2400/1*_NdnljMqi8L2_lAYwH3JDQ.gif)
</div>


# Latent Dirichlet Allocation 


<center>
![](Capture1.png){width=60%}
</center>


<div class='left' style='float:left;width:52%'>
1. For $j = 1$ : $M$ (le nombre de documents)
     * Paramétrage de la distribution des sujets pour chaque document
     * $\theta_j$ ~ Dirichlet ($\alpha$)
2. For $i = 1$ : $K$ (le nombre de sujets)
     * Paramétrage de la distribution des mots pour chaque sujet
     * $\varphi_i$ ~ Dirichlet ($\beta$)
</div>

<div class='right' style='float:right;width:48%'>
  3. For $t = 1$ : $N$ (le nombre de mots dans le document $j$)
     - Choisir le Topic pour le mot $t$
     - $z_{j,t}$ ~ Multinomial ($\theta_j$)
     - choisir le mot à partir de la distribution de mots pour chaque sujet $z$ 
     - $\omega_{j,t}$ ~ Multinomial ($\varphi^{(z_{j,t})}$)
</div>


# Latent Dirichlet Allocation 

<center>
![](Capture1.png){width=60%}
</center>

<center>
![ghtr](Capture2.png){width=65%}
</center>

# Latent Dirichlet Allocation 

<div class='left' style='float:left;width:63%'>
**Steps!**

1 : Chargement des données

2 : Nettoyage des données

3 : Modélisation d'expressions : bi-grammes et tri-grammes

4 : Transformation de données : Corpus et Dictionnaire

5 : Modèle de base

6 : Optimisation des hyperparamètres

7 : Modèle final

8 : Visualisation des résultats
</div>
<div class='right' style='float:right;width:37%'>
**Modules!**
```{python, eval=FALSE, size="large"}
import gensim
import gensim.corpora as corpora
from gensim.utils import simple_preprocess
from gensim.models import CoherenceModel
# spacy pour lemmatization
import spacy
# Plotting tools
import pyLDAvis
import pyLDAvis.gensim_models as gensimvis
import matplotlib.pyplot as plt
```
</div>

# Hyperparmeter Tuning

- Module : *gensim.models.LdaMulticore*
<center> 
![Demande d'affichage des notifications par FB](LDATuning.png){width=55%}
<center>

- Nombre de sujet $K$
- Hyperparamètre Dirichlet $\alpha$ : Densité de Document-Topic
- Hyperparamètre Dirichlet $\beta$: Densité de mot-Topic

# Visualisation  de LDA (*pyLDAvis*)

<center>
```{r,fig.align="center", echo=FALSE}
#stargazer::stargazer(lm0, lm1, lm2, type = "html", title = "Results", out = "./pp.html")
shiny::includeHTML("lda.html")
```
</center>

# Conclusion

<font size='15'> 
 
- Efficacité de Selénium : Glassdoor a une API disponible, mais pas pour le contenu qu'on recherche

- On a construit un modèle LDA à l'aide de Gensim et optimisé les hyperparamètres LDA.

- Comparaison de l'utilité des deux méthodes *NLP* : *LDA* (non-supervisé) et *Naive Bayes* (supervisé) 

-  Inéfficacité de LDA pour les textes courts (tweets ou des commentaires) - un meilleur choix : *Biterm Topic Model
*


 </font size> 

